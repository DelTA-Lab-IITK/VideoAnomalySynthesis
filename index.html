<!-- 
<center> <span style="font-size:42px"> ** Webpage Under Construction **</span> </center>
<br>
<br>

<center> <span style="font-size:42px">Unsupervised Synthesis of Anomalies in Video: Transforming the Normal</span> </center>
<center> <span style="font-size:25px">Abhishek Joshi   Vinay P. Namboodiri</span> </center>
<center> <span style="font-size:18px">IIT Kanpur</span> </center>

<br>
<center> 	<img class="rounded" src="./imgs/Bayesian.png" height="300px">  <br>	</center>

<hr>
<h1> <center> Abstract </center> </h1>
<p align="justify"> Abnormal activity recognition requires detection of occurrence of anomalous events that suffer from a severe imbalance in data. In a video, normal is used to describe activities that conform to usual events while the irregular events which do not conform to the normal are referred to as abnormal. It is far more common to observe normal data than to obtain abnormal data in visual surveillance. In this paper, we propose an approach where we can obtain abnormal data by transforming normal data. This is a challenging task that is solved through a multi-stage pipeline approach. We utilize a number of techniques from unsupervised segmentation in order to synthesize new samples of data that are transformed from an existing set of normal examples. An incrementally trained Bayesian convolutional neural network (CNN) is used to carefully select the set of abnormal samples that can be added. Finally through this synthesis approach we obtain a comparable set of abnormal samples that can be used for training the CNN for the classification of normal vs abnormal samples. We show that this method generalizes to multiple settings by evaluating it on two real world datasets and achieves improved performance over other probabilistic techniques that have been used in the past for this task.
</p>
<hr>

<h1> <center> DEMO [Video Link + Gif to be added] </center> </h1>
 -->


<!-- saved from url=(0037)https://junyanz.github.io/BicycleGAN/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Toward Multimodal Image-to-Image Translation_files/analytics.js"></script><script src="./Toward Multimodal Image-to-Image Translation_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1000px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>


  
		<title>Unsupervised Synthesis of Anomalies in Video: Transforming the Normal</title>
<!-- 		<meta property="og:image" content="https://junyanz.github.io/BicycleGAN/index_files/teaser_fb2.jpg"> -->
		<meta property="og:title" content="Unsupervised Synthesis of Anomalies in Video: Transforming the Normal. In IJCNN, 2019.">
  </head>

  <body>
    <br>
          <center>
          	<span style="font-size:42px">Unsupervised Synthesis of Anomalies in Video: Transforming the Normal</span><br>
	  		  <table align="center" width="500px">

	  		  <br>
	  		  <!-- <table align=center width=540px> -->
	  			  <tbody><tr>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="">Abhishek Joshi</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px"><a href="">Vinay P. Namboodiri</a></span>
		  		  		</center>
		  		  	  </td>
	  	             
	  			  </tr>
			  </tbody></table>
	  		  <table align="center" width="1000px">
	  			  <tbody><tr>
	  	              <td align="center" width="100px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
				          	<span style="font-size:18px">Indian Institute of Technology Kanpur</span>
		  		  		</center>
		  		  	  </td>
	  	        
	  	              <td align="center" width="100px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table>
<!-- 	  		  <table align="center" width="1000px">
	  			  <tbody><tr>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
	  						<span style="font-size:22px">Code <a href="https://github.com/junyanz/BicycleGAN"> [GitHub]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="225px">
	  					<center>
	  						<span style="font-size:22px">NIPS 2017<a href="https://arxiv.org/abs/1711.11586"> [Paper]</a></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="275px">
	  					<center>
				          	<span style="font-size:18px"></span>
		  		  		</center>
		  		  	  </td>
			  </tr></tbody></table> -->
          </center>
          <br>
  		  <table align="center" width="900px">
  			  <tbody><tr>
  	              <td width="400px">
  	                	<center><img class="rounded" src="./Toward Multimodal Image-to-Image Translation_files/Bayesian.png" height="300px"></center>
  	                	<br>
  	              </td>
  	              </tr>
  	              </tbody></table>

  		  <br>
		  <hr>

  		  <center><h1>Abstract</h1></center><p align="justify">Abnormal activity recognition requires detection of occurrence of anomalous events that suffer from a severe imbalance in data. In a video, normal is used to describe activities that conform to usual events while the irregular events which do not conform to the normal are referred to as abnormal. It is far more common to observe normal data than to obtain abnormal data in visual surveillance. In this paper, we propose an approach where we can obtain abnormal data by transforming normal data. This is a challenging task that is solved through a multi-stage pipeline approach. We utilize a number of techniques from unsupervised segmentation in order to synthesize new samples of data that are transformed from an existing set of normal examples. An incrementally trained Bayesian convolutional neural network (CNN) is used to carefully select the set of abnormal samples that can be added. Finally through this synthesis approach we obtain a comparable set of abnormal samples that can be used for training the CNN for the classification of normal vs abnormal samples. We show that this method generalizes to multiple settings by evaluating it on two real world datasets and achieves improved performance over other probabilistic techniques that have been used in the past for this task.</p><br><hr><table align="center" width="900px">
	  		 	
	  		 	<tbody><tr>
		            <!-- <td align=center width=600px> -->
					
					<!-- </td> -->
				</tr>
  		  
		  

  		  </tbody></table><center><h1>Demo Video</h1></center><table align="center" width="900px">
	  		 	
  			  <tbody><tr>
<!--   	              <td width=400px>
  					<center>
  	                	<img class="rounded" src = "./index_files/teaser_v3.jpg" height="275px"></img>
  	                	<br>
					</center>
  	              </td> -->

  	              <tr>  		    <td> <center><img href="" src="http://deltalab.iitk.ac.in/image/gif.gif" alt=""></center></td>
</tr>

		  		  </tr></tbody></table><table align="center" width="600px">
		  		    <tbody><tr><!-- 
		              <td align="center" width="600px">
						<iframe width="800" height="450" src="./Toward Multimodal Image-to-Image Translation_files/JvGysD2EFhw.html" frameborder="0" allowfullscreen=""></iframe>
					  </td>
					</tr>
				  </tbody></table> -->
				<br>
                
  		  

        <center><a href="https://cse.iitk.ac.in/users/abhishekjoshi/AbnormalDemo/" target="_blank"> [Watch Demo] </a> </center>
  		  <br>

		  <hr>

 		<center><h1>Synthesized Samples</h1></center>

  		  <table align="center" width="600px">
  		    <tbody><tr>
              <td align="center" width="600px">
        		<!-- <a href="./index_files/legacy_v4.jpg"><img class="rounded"  src = "./index_files/legacy_v4_small.jpg" width = "800px"></a><br> -->
        		<img class="rounded" src="./Toward Multimodal Image-to-Image Translation_files/VideoAnomalySamples.png" width="400px"><br>

				<span style="font-size:16px"></span>
			  </td>
			</tr>
		  </tbody></table>
  		  <!-- <table align="center" width="800px">
  		  	<tbody><tr>
		        <td align="center" width="800px">
					<span style="font-size:16px"><b>Unsynchronized z</b>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/facades_random_z_20_20/">[labels → facades]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/shoes_random_z_20_20/">[edges → shoes]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/handbags_random_z_20_20/">[edges → handbags]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/night2day_random_z/">[night → day]</a>
					</span>
				</td>
			</tr>
  		  	<tr>
		        <td align="center" width="800px">
					<span style="font-size:16px"><b>Synchronized z</b>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/facades_fixed_z_100_6/">[labels → facades]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/shoes_fixed_z_50_6/">[edges → shoes]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/handbags_fixed_z_100_6/">[edges → handbags]</a>
					<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN//web_arxiv/night2day_random_z/">[night → day]</a>
					</span>
				</td>
			</tr>
		</tbody></table> -->

		<br>
		<hr>
<!-- 
		<center><h1> Exploring the Latent Space</h1><center>
		<table align="center" width="800px">
			<tbody><tr>
				<td align="center">
					<a href="./Toward Multimodal Image-to-Image Translation_files/day2night.gif"><img src="./Toward Multimodal Image-to-Image Translation_files/day2night.gif"></a><br>
					<span style="font-size:16px"></span>
				</td>
			</tr>
		</tbody></table>

		<hr>

 		<center><h1>Try the BicycleGAN model</h1></center>

  		  <table align="center" width="900px">
  			  <tbody><tr>
  	                <td align="center" width="900px">
  					<center>
						  </center></td><td><a href="https://github.com/junyanz/BicycleGAN"><img class="round" style="width:900px" src="./Toward Multimodal Image-to-Image Translation_files/network2.jpg"></a></td>
	  		  		
	  		  		
			  </tr>
		  </tbody></table>

		  <br>

  		  <center>
				<span style="font-size:28px"><a href="https://github.com/junyanz/BicycleGAN">[GitHub]</a>
			  <br>
			  </span></center><table align="center" width="800px">
			  <tbody><tr></tr>
		  </tbody></table> -->
		  <br>


					    <center><h1> [<a href="https://arxiv.org/pdf/1904.06633.pdf"> Paper </a>]   [<a href="https://arxiv.org/abs/1904.06633"> arXiv </a>] </h1></center><table align="center" width="600" px="">
	 		
  			  <tbody><tr>
				  <td><a href=""><img class="layered-paper-big" style="height:175px" src="./Toward Multimodal Image-to-Image Translation_files/page1.jpg"></a></td>
				  <td><span style="font-size:12pt">A. Joshi, V. P. Namboodiri<br>
				  <b><span style="font-size:12pt">Unsupervised Synthesis of Anomalies in Video: Transforming the Normal</span></b><br>
				  <!-- <span style="font-size:12pt">In IJCNN, 2019.  -->
				  <!-- (hosted on <a href="https://arxiv.org/abs/1711.11586">arXiv</a>) -->
				  </span>
				  <span style="font-size:4pt"><a href="https://junyanz.github.io/BicycleGAN/"><br></a>
				  </span>
				  </td>
  	              
              </tr>
  		  </tbody></table>
		  <br>

		  <table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<a href="">[Bibtex]</a>
  	              </center></span></td>
              </tr>
  		  </tbody></table>

  		  <br>
<!-- 		  <hr>
		 <center><h1>Poster</h1></center><table align="center" width="200" px="">
	 		
  			  <tbody><tr>
				  <td><a href="https://junyanz.github.io/BicycleGAN/index_files/poster_nips_v3.pdf"><img class="paper-big" style="width:600px" src="./Toward Multimodal Image-to-Image Translation_files/poster_teaser.png"></a></td>
              </tr>
  		  </tbody></table>
		  <br>

		  <table align="center" width="600px">
			  <tbody><tr>
				  <td><span style="font-size:24pt"><center>
				  	<a href="https://junyanz.github.io/BicycleGAN/index_files/poster_nips_v3.pdf">[PDF]</a>
  	              </center></span></td>
              </tr>
  		  </tbody></table>

  		  <br>
		  <hr>

  		  <table align="center" width="1000px">
  			  <tbody><tr>
  	              <td width="400px">
  					<left>
	  		  <center><h1>Related Work</h1></center>
					Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei. A. Efros. <b>Image-to-image translation with conditional adversarial networks.</b> In CVPR, 2017. <a href="https://arxiv.org/pdf/1611.07004v1.pdf">[PDF]</a><a href="https://phillipi.github.io/pix2pix/"> [Website]</a><br>

					<br>

					Jun-Yan Zhu*, Taesung Park*, Phillip Isola, Alexei A. Efros. <b>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.</b> In ICCV, 2017. <a href="https://arxiv.org/abs/1703.10593"> [PDF]</a><a href="https://junyanz.github.io/CycleGAN/"> [Website]</a><br>

					<br>

			</left>
		</td>
			 </tr>
		</tbody></table>
		<hr>

  		  <table align="center" width="1000px">
  			  <tbody><tr>
  	              <td width="400px">
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
				<p align="justify">We thank Phillip Isola and Tinghui Zhou for helpful discussions. This work wassupported in part by Adobe Inc., DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS-1633310, IIS-1427425, IIS-1212798, the Berkeley Artificial Intelligence Research (BAIR) Lab,and hardware donations from NVIDIA. JYZ is supported by the Facebook Graduate Fellowship, RZ by the Adobe Research Fellowship, and DP by the NVIDIA Graduate Fellowship.</p>
			</left>
		</td>
			 </tr>
		</tbody></table>
 -->
		<br>
		<hr>
	  
		<center><h1>Acknowledgements</h1></center>
					    <p align="center">We acknowledge the travel support from <a target="_blank" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research India</a>. 
					    <br> We thank the members of <a target="_blank" href="http://deltalab.iitk.ac.in/"> DelTA Lab </a> for helpful discussions.</p>
					  
					    
		<br> 
		
		    <center><h1>Citation</h1></center>
		    <p align="left"> <br>
		    If you find this useful for your research, please use the following. <br>
		    @InProceedings{abhishekjoshi_2019_IJCNN, <br>
  		   title={Video-to-Video Synthesis}, <br>
  		   author={Joshi, Abhishek and Namboodiri, Vinay P}, <br>
                   booktitle={International Joint Conference on Neural Networks (IJCNN)}, <br> 
		    year={2019} <br>
		    }
					    
		    </p>

		<br> 
		<br>
					    
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-137361055-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-137361055-1');
</script>



</center></center></body></html>
